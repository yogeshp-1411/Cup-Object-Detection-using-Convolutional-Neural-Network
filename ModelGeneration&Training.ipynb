{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Activation, MaxPool2D, Flatten\n",
    "\n",
    "input_shape = (-1, 70, 70, 1)\n",
    "model = Sequential([\n",
    "    Conv2D(32, (1,1), padding = \"valid\", activation='relu', input_shape = input_shape[1:]),\n",
    "    MaxPool2D((2,2), (2,2)),    \n",
    "    Flatten(),\n",
    "    Dense(units=32, activation = 'relu'),\n",
    "    Dense(units=1, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 70, 70, 32)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 35, 35, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 39200)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1254432   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,254,529\n",
      "Trainable params: 1,254,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm as tqdm\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "DATADIR = \"C:/Users/Yogesh/CupDetector\"\n",
    "CATEGORIES = [\"Others\", \"Cup\"]\n",
    "IMG_SIZE = 70\n",
    "training_data = []\n",
    "\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR, category)\n",
    "        class_num =  CATEGORIES.index(category)\n",
    "\n",
    "        for img in os.listdir(path):\n",
    "            img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "            new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "            training_data.append([new_array, class_num])\n",
    "        \n",
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       ...,\n",
      "       [197, 213, 220, ..., 255, 255, 255],\n",
      "       [189, 196, 194, ..., 255, 255, 255],\n",
      "       [255, 254, 255, ..., 255, 255, 255]], dtype=uint8), 1]\n"
     ]
    }
   ],
   "source": [
    "print(training_data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for sample in training_data[:10]:\n",
    "    print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[199]\n",
      "   [203]\n",
      "   [201]\n",
      "   ...\n",
      "   [198]\n",
      "   [193]\n",
      "   [199]]\n",
      "\n",
      "  [[199]\n",
      "   [201]\n",
      "   [202]\n",
      "   ...\n",
      "   [191]\n",
      "   [201]\n",
      "   [197]]\n",
      "\n",
      "  [[201]\n",
      "   [203]\n",
      "   [202]\n",
      "   ...\n",
      "   [200]\n",
      "   [200]\n",
      "   [200]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[174]\n",
      "   [178]\n",
      "   [201]\n",
      "   ...\n",
      "   [198]\n",
      "   [172]\n",
      "   [200]]\n",
      "\n",
      "  [[186]\n",
      "   [189]\n",
      "   [195]\n",
      "   ...\n",
      "   [183]\n",
      "   [185]\n",
      "   [196]]\n",
      "\n",
      "  [[186]\n",
      "   [192]\n",
      "   [189]\n",
      "   ...\n",
      "   [190]\n",
      "   [187]\n",
      "   [188]]]]\n"
     ]
    }
   ],
   "source": [
    "train_img = []\n",
    "train_labels = []\n",
    "\n",
    "for feature, label in training_data:\n",
    "    train_img.append(feature)\n",
    "    train_labels.append(label)\n",
    "\n",
    "print(train_img[0].reshape(-1, IMG_SIZE, IMG_SIZE, 1))\n",
    "\n",
    "train_img = np.array(train_img).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "train_labels = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_out = open(\"train_img.pickle\", \"wb\")\n",
    "pickle.dump(train_img, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"train_labels.pickle\", \"wb\")\n",
    "pickle.dump(train_labels, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#while loading the dataset:\n",
    "pickle_in = open(\"train_img.pickle\", \"rb\")\n",
    "train_img = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"train_labels.pickle\", \"rb\")\n",
    "train_labels = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm as tqdm\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "DATADIR = \"C:/Users/Yogesh/CupDetector\"\n",
    "CATEGORIES = [\"Others\", \"Cup\"]\n",
    "IMG_SIZE = 70\n",
    "test_data = []\n",
    "\n",
    "def create_test_data():\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR, category)\n",
    "        class_num =  CATEGORIES.index(category)\n",
    "\n",
    "        for img in os.listdir(path):\n",
    "            img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "            new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "            test_data.append([new_array, class_num])\n",
    "        \n",
    "create_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-129-03266500238c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(test_data[60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for sample in training_data[:10]:\n",
    "    print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[255]\n",
      "   [255]\n",
      "   [255]\n",
      "   ...\n",
      "   [255]\n",
      "   [255]\n",
      "   [255]]\n",
      "\n",
      "  [[255]\n",
      "   [255]\n",
      "   [255]\n",
      "   ...\n",
      "   [255]\n",
      "   [255]\n",
      "   [255]]\n",
      "\n",
      "  [[255]\n",
      "   [255]\n",
      "   [255]\n",
      "   ...\n",
      "   [255]\n",
      "   [255]\n",
      "   [255]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[255]\n",
      "   [255]\n",
      "   [255]\n",
      "   ...\n",
      "   [255]\n",
      "   [255]\n",
      "   [255]]\n",
      "\n",
      "  [[255]\n",
      "   [255]\n",
      "   [255]\n",
      "   ...\n",
      "   [255]\n",
      "   [255]\n",
      "   [255]]\n",
      "\n",
      "  [[255]\n",
      "   [255]\n",
      "   [255]\n",
      "   ...\n",
      "   [255]\n",
      "   [255]\n",
      "   [255]]]]\n"
     ]
    }
   ],
   "source": [
    "test_img = []\n",
    "test_labels = []\n",
    "\n",
    "for feature, label in test_data:\n",
    "    test_img.append(feature)\n",
    "    test_labels.append(label)\n",
    "\n",
    "print(test_img[0].reshape(-1, IMG_SIZE, IMG_SIZE, 1))\n",
    "\n",
    "test_img = np.array(test_img).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_out = open(\"test_img.pickle\", \"wb\")\n",
    "pickle.dump(test_img, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"test_labels.pickle\", \"wb\")\n",
    "pickle.dump(test_labels, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#while loading the dataset:\n",
    "pickle_in = open(\"test_img.pickle\", \"rb\")\n",
    "test_img = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"test_labels.pickle\", \"rb\")\n",
    "test_labels = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "\n",
    "model.compile(optimizer= Adam(learning_rate = 0.0001), loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5 samples, validate on 21 samples\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 0s 15ms/sample - loss: 6.0997 - accuracy: 0.6000 - val_loss: 5.0831 - val_accuracy: 0.6667\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 14ms/sample - loss: 6.0997 - accuracy: 0.6000 - val_loss: 5.0831 - val_accuracy: 0.6667\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 15ms/sample - loss: 6.0997 - accuracy: 0.6000 - val_loss: 5.0831 - val_accuracy: 0.6667\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 15ms/sample - loss: 6.0997 - accuracy: 0.6000 - val_loss: 5.0831 - val_accuracy: 0.6667\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 14ms/sample - loss: 6.0997 - accuracy: 0.6000 - val_loss: 5.0831 - val_accuracy: 0.6667\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 12ms/sample - loss: 6.0997 - accuracy: 0.6000 - val_loss: 5.0831 - val_accuracy: 0.6667\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 14ms/sample - loss: 6.0997 - accuracy: 0.6000 - val_loss: 5.0831 - val_accuracy: 0.6667\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 13ms/sample - loss: 6.0997 - accuracy: 0.6000 - val_loss: 5.0831 - val_accuracy: 0.6667\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 13ms/sample - loss: 6.0997 - accuracy: 0.6000 - val_loss: 5.0831 - val_accuracy: 0.6667\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 13ms/sample - loss: 6.0997 - accuracy: 0.6000 - val_loss: 5.0831 - val_accuracy: 0.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2566ee3f3c8>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img = train_img/255.0\n",
    "model.fit(train_img, train_labels, steps_per_epoch = 1, epochs = 10, validation_split = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 17ms/sample - loss: 5.2786 - accuracy: 0.6538\n",
      "Test loss: 5.278582572937012 - Test Accuracy: 0.6538461446762085\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_img, test_labels)\n",
    "print(\"Test loss: {0} - Test Accuracy: {1}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "def prepare(filepath):\n",
    "    IMG_SIZE = 70  \n",
    "    img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "    return new_array.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "\n",
    "prediction = model.predict(prepare(\"C:/Users/Yogesh/CupDetector/Cup/cup3.jpg\"))\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cup detected\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "def prepare(filepath):\n",
    "    IMG_SIZE = 70  \n",
    "    img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "    return new_array.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "\n",
    "prediction = model.predict(prepare(\"C:/Users/Yogesh/CupDetector/Others/obj3.jpg\"))\n",
    "\n",
    "if prediction == 0:\n",
    "    print(\"Cup not detected\")\n",
    "else:\n",
    "    print(\"Cup detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "abc = 0\n",
    "plt.imshow(test_img[abc], cmap = 'gray')\n",
    "plt.show()\n",
    "print(test_labels[abc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
